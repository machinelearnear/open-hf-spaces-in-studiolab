{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/machinelearnear/open-hf-spaces-in-studiolab/blob/main/run_google_colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tA1VyFSQOAjd",
    "tags": []
   },
   "source": [
    "# Demo: `XYZ` a travÃ©s de `dðŸ§¨ffusers` y Hugging Face Spaces\n",
    "\n",
    "`machinelearnear` ðŸ§‰ðŸ¤– > https://www.youtube.com/c/machinelearnear\n",
    "---\n",
    "**Referencias**\n",
    "- https://huggingface.co/spaces/Fantasy-Studio/Paint-by-Example\n",
    "- https://github.com/machinelearnear/open-hf-spaces-in-studiolab\n",
    "- [Paint by Example: Exemplar-based Image Editing with Diffusion Models (ArXiv)](https://arxiv.org/abs/2211.13227)\n",
    "- [Paint by Example: Exemplar-based Image Editing with Diffusion Models (GitHub)](https://github.com/Fantasy-Studio/Paint-by-Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9RkjQ99OIU_"
   },
   "source": [
    "## Configurar entorno de Hugging Face Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rmBjvPVHO1fs"
   },
   "outputs": [],
   "source": [
    "#@title Un poco de cÃ³digo para configurar todo, clonar el repo, instalar las librerias, etc.\n",
    "# source: https://www.youtube.com/c/machinelearnear\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "except ImportError:\n",
    "    subprocess.run([\"pip\", \"install\", \"gradio\"])\n",
    "    import gradio as gr\n",
    "\n",
    "from os.path import exists as path_exists\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "class RepoHandler:\n",
    "    def __init__(self, repo_url: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the RepoHandler class with the provided repository URL and requirements file.\n",
    "        Args:\n",
    "        - repo_url: URL of the git repository to clone.\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.is_google_colab = False\n",
    "        self.fix_for_gr, self.fix_for_st = None, None\n",
    "        self.app_file = \"app.py\"\n",
    "        if 'google.colab' in str(get_ipython()):\n",
    "            print(f'{bold}Running on: \"Google Colab\"{unbold}')\n",
    "            self.is_google_colab = True\n",
    "        else:\n",
    "            print(f'{bold}Running on: Local or \"SM Studio Lab\"{unbold}')\n",
    "        self.repo_url = repo_url\n",
    "        self.repo_name = self.repo_url.split('/')[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        if os.path.exists(self.repo_name):\n",
    "            return self.retrieve_readme(f'{self.repo_name}/README.md')\n",
    "        else:\n",
    "            print(f\"{bold}The repo '{self.repo_name}' has not been cloned yet.{unbold}\")\n",
    "            return None\n",
    "            \n",
    "    def retrieve_readme(self, filename) -> Dict:\n",
    "        readme = {}\n",
    "        if path_exists(filename):\n",
    "            with open(filename) as f:\n",
    "                for line in f:\n",
    "                    if not line.find(':') > 0 or 'Check' in line: continue\n",
    "                    (k,v) = line.split(':')\n",
    "                    readme[(k)] = v.strip().replace('\\n','')\n",
    "        else:\n",
    "            print(f\"{bold}No 'readme.md' file{unbold}\")\n",
    "            \n",
    "        return readme\n",
    "        \n",
    "    def clone_repo(self, overwrite=False) -> None:\n",
    "        \"\"\"\n",
    "        Clone the git repository specified in the repo_url attribute.\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Check if repository has already been cloned locally\n",
    "        if overwrite and os.path.exists(self.repo_name): \n",
    "            try:\n",
    "                shutil.rmtree(self.repo_name)\n",
    "            except OSError as e:\n",
    "                print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "        if os.path.exists(self.repo_name):\n",
    "            print(f\"{bold}Repository '{self.repo_name}' has already been cloned.{unbold}\")\n",
    "        else:\n",
    "            print(f\"{bold}Cloning repo... may take a few minutes... remember to set your Space to 'public'...{unbold}\")\n",
    "            subprocess.run([\"apt-get\", \"install\", \"git-lfs\"])\n",
    "            subprocess.run([\"git\", \"lfs\", \"install\", \"--system\", \"--skip-repo\"])\n",
    "            subprocess.run([\"git\", \"clone\", self.repo_url])\n",
    "\n",
    "    def install_requirements(self, requirements_file: str = None, install_xformers: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Install the requirements specified in the requirements_file attribute.\n",
    "        \n",
    "        Args:\n",
    "        - requirements_file: Name of the file containing the requirements to install. This file must be \n",
    "        located in the root directory of the repository. Defaults to \"requirements.txt\".\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        if not requirements_file: requirements_file = f\"{self.repo_name}/requirements.txt\"\n",
    "        \n",
    "        # install requirements\n",
    "        print(f\"{bold}Installing requirements... may take a few minutes...{unbold}\")\n",
    "        subprocess.run([\"pip\", \"install\", \"-r\", requirements_file])\n",
    "        if install_xformers: self.install_xformers()\n",
    "\n",
    "    def run_web_demo(self, aws_domain=None, aws_region=None) -> None:\n",
    "        \"\"\"\n",
    "        Launch the Gradio or Streamlit web demo for the cloned repository.\n",
    "        Works with Google Colab or SageMaker Studio Lab.\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        if torch.cuda.is_available(): print(f\"{bold}Using: {unbold}{self.get_gpu_memory_map()}\")\n",
    "        else: print(f\"{bold}Not using the GPU{unbold}\")\n",
    "        \n",
    "        readme = self.__str__()\n",
    "        self.app_file = readme[\"app_file\"]\n",
    "        print(f\"{bold}Demo: `{readme['title']}`{newline}{unbold}\")\n",
    "        print(f\"{bold}Downloading models... might take up to 10 minutes to finish...{unbold}\")\n",
    "        print(f\"{bold}Once finished, click the link below to open your application (in SM Studio Lab):{newline}{unbold}\")\n",
    "        if all([aws_domain, aws_region]):\n",
    "              print(f'{bold}https://{aws_domain}.studio.{aws_region}.sagemaker.aws/studiolab/default/jupyter/proxy/6006/{unbold}')\n",
    "                \n",
    "        self.unset_environment_variables()\n",
    "        \n",
    "        if readme[\"sdk\"] == 'gradio':\n",
    "            gr.close_all()\n",
    "            if not self.is_google_colab:\n",
    "                !export GRADIO_SERVER_PORT=6006 && cd $self.repo_name && python $self.app_file\n",
    "                # os.system(f'export GRADIO_SERVER_PORT=6006 && cd {self.repo_name} && python {readme[\"app_file\"]}')\n",
    "            else:\n",
    "                new_filename = self.replace_gradio_launcher(f'{self.repo_name}/{readme[\"app_file\"]}')\n",
    "                !cd $self.repo_name && python $new_filename\n",
    "                # os.system(f'cd {self.repo_name} && python {readme[\"app_file\"]}')\n",
    "        elif readme[\"title\"] == 'streamlit':\n",
    "            if not self.is_google_colab:\n",
    "                !cd $self.repo_name && streamlit run $self.app_file --server.port 6006\n",
    "                # os.system(f'cd {self.repo_name} && streamlit run {readme[\"app_file\"]} --server.port 6006')\n",
    "            else:\n",
    "                !cd $self.repo_name && streamlit run $self.app_file\n",
    "                # os.system(f'cd {self.repo_name} && streamlit run {readme[\"app_file\"]}')\n",
    "        else:\n",
    "            print('This notebook will not work with static apps hosted on \"Spaces\"')\n",
    "\n",
    "    def get_gpu_memory_map(self) -> Dict[str, int]:\n",
    "        \"\"\"Get the current gpu usage.\n",
    "        Return:\n",
    "            A dictionary in which the keys are device ids as integers and\n",
    "            values are memory usage as integers in MB.\n",
    "        \"\"\"\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total,memory.free\", \"--format=csv,noheader\",],\n",
    "            encoding=\"utf-8\",\n",
    "            # capture_output=True,          # valid for python version >=3.7\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,  # for backward compatibility with python version 3.6\n",
    "            check=True,\n",
    "        )\n",
    "        # Convert lines into a dictionary, return f\"{}\"\n",
    "        gpu_memory = [x for x in result.stdout.strip().split(os.linesep)]\n",
    "        gpu_memory_map = {f\"gpu_{index}\": memory for index, memory in enumerate(gpu_memory)}\n",
    "        \n",
    "        return gpu_memory_map\n",
    "\n",
    "    def replace_gradio_launcher(self, old_filename) -> str:\n",
    "        # Read the contents of the file\n",
    "        with open(old_filename, \"r\") as f:\n",
    "            contents = f.read()\n",
    "        # Define the regular expression pattern\n",
    "        pattern = r\"\\.launch\\((.*?)\\)\"\n",
    "        # Use the sub method to replace the text\n",
    "        contents = re.sub(pattern, \".launch(share=True)\", contents)\n",
    "        # Write the modified contents back to the file\n",
    "        new_filename = Path(old_filename).parent / f\"{Path(old_filename).stem}_modified.py\"\n",
    "        with open(new_filename, \"w\") as f:\n",
    "            f.write(contents)\n",
    "\n",
    "        return new_filename.name\n",
    "    \n",
    "    def unset_environment_variables(self) -> None:\n",
    "        os.unsetenv(\"SHARED_UI\")\n",
    "        os.environ.pop(\"SHARED_UI\", None)\n",
    "        \n",
    "        os.unsetenv(\"IS_SHARED\")\n",
    "        os.environ.pop(\"IS_SHARED\", None)\n",
    "\n",
    "    def install_xformers(self) -> None:\n",
    "        from subprocess import getoutput\n",
    "        from IPython.display import HTML\n",
    "        from IPython.display import clear_output\n",
    "        import time\n",
    "\n",
    "        subprocess.run([\"pip\", \"install\", \"-U\", \"--pre\", \"triton\"])\n",
    "\n",
    "        s = getoutput('nvidia-smi')\n",
    "        if 'T4' in s: gpu = 'T4'\n",
    "        elif 'P100' in s: gpu = 'P100'\n",
    "        elif 'V100' in s: gpu = 'V100'\n",
    "        elif 'A100' in s: gpu = 'A100'\n",
    "\n",
    "        while True:\n",
    "            try: \n",
    "                gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "            print(f'{bold} Seems that your GPU is not supported at the moment.{unbold}')\n",
    "            time.sleep(5)\n",
    "\n",
    "        if (gpu=='T4'): \n",
    "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
    "        elif (gpu=='P100'): \n",
    "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
    "        elif (gpu=='V100'): \n",
    "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
    "        elif (gpu=='A100'): \n",
    "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
    "\n",
    "        subprocess.run([\"pip\", \"install\", \"-q\", precompiled_wheels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioW5IDk4PXg4"
   },
   "source": [
    "## Ya tenemos todo listo, empezar el proceso! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvACWIthPn4P",
    "outputId": "9ce736c1-5ef8-4737-e48b-8b27fa0c6797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRunning on: Local or \"SM Studio Lab\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "app = RepoHandler(\n",
    "    repo_url='https://huggingface.co/spaces/hysts/LoRA-SD-training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfE0Zx-2PvUd",
    "outputId": "073e29e0-40ba-4d47-d816-d0dceb98b6a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCloning repo... may take a few minutes... remember to set your Space to 'public'...\u001b[0m\n",
      "WARNING: current user is not root/admin, system install is likely to fail.\n",
      "Git LFS initialized.\n",
      "\u001b[1mInstalling requirements... may take a few minutes...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Cloning into 'LoRA-SD-training'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.15.0\n",
      "  Using cached accelerate-0.15.0-py3-none-any.whl (191 kB)\n",
      "Collecting bitsandbytes==0.35.4\n",
      "  Using cached bitsandbytes-0.35.4-py3-none-any.whl (62.5 MB)\n",
      "Collecting diffusers==0.10.2\n",
      "  Using cached diffusers-0.10.2-py3-none-any.whl (503 kB)\n",
      "Collecting ftfy==6.1.1\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: Pillow==9.3.0 in /home/studio-lab-user/.conda/envs/machinelearnear-hf-spaces/lib/python3.9/site-packages (from -r LoRA-SD-training/requirements.txt (line 5)) (9.3.0)\n",
      "Collecting torch==1.13.0\n",
      "  Using cached torch-1.13.0-cp39-cp39-manylinux1_x86_64.whl (890.2 MB)\n",
      "Collecting torchvision==0.14.0\n",
      "  Using cached torchvision-0.14.0-cp39-cp39-manylinux1_x86_64.whl (24.3 MB)\n",
      "Collecting transformers==4.25.1\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting triton==2.0.0.dev20220701\n",
      "  Using cached triton-2.0.0.dev20220701-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Collecting xformers==0.0.13\n",
      "  Using cached xformers-0.0.13.tar.gz (292 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— python setup.py egg_info did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [6 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"/tmp/pip-install-mn53dmo_/xformers_6e97281d71ee42fa9d4d5da239d65927/setup.py\", line 18, in <module>\n",
      "          import torch\n",
      "      ModuleNotFoundError: No module named 'torch'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Ã— Encountered error while generating package metadata.\n",
      "â•°â”€> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "app.clone_repo(overwrite=False)\n",
    "app.install_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tkTJI0SPvv5",
    "outputId": "e69a7612-5556-40c9-ada8-3bbac606f8a0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_web_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 106\u001b[0m, in \u001b[0;36mRepoHandler.run_web_demo\u001b[0;34m(self, aws_domain, aws_region)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_web_demo\u001b[39m(\u001b[38;5;28mself\u001b[39m, aws_domain\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, aws_region\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    Launch the Gradio or Streamlit web demo for the cloned repository.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Works with Google Colab or SageMaker Studio Lab.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available(): \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mUsing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munbold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gpu_memory_map()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mNot using the GPU\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munbold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "app.run_web_demo()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "machinelearnear_sd_paint_by_example.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
