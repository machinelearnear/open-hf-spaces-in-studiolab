{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "machinelearnear_sd_paint_by_example.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Demo: \"Paint by Example ðŸŽ¨\" a travÃ©s de `dðŸ§¨ffusers` y Hugging Face Spaces\n",
        "\n",
        "`machinelearnear` ðŸ§‰ðŸ¤– > https://www.youtube.com/c/machinelearnear\n",
        "---\n",
        "**Referencias**\n",
        "- https://huggingface.co/spaces/Fantasy-Studio/Paint-by-Example\n",
        "- https://github.com/machinelearnear/open-hf-spaces-in-studiolab\n",
        "- [Paint by Example: Exemplar-based Image Editing with Diffusion Models (ArXiv)](https://arxiv.org/abs/2211.13227)\n",
        "- [Paint by Example: Exemplar-based Image Editing with Diffusion Models (GitHub)](https://github.com/Fantasy-Studio/Paint-by-Example)"
      ],
      "metadata": {
        "id": "tA1VyFSQOAjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurar entorno de Hugging Face Spaces"
      ],
      "metadata": {
        "id": "j9RkjQ99OIU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Un poco de cÃ³digo para configurar todo, clonar el repo, instalar las librerias, etc.\n",
        "# source: https://www.youtube.com/c/machinelearnear\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import subprocess\n",
        "import re\n",
        "\n",
        "try:\n",
        "  import gradio as gr\n",
        "except ImportError:\n",
        "  subprocess.run([\"pip\", \"install\", \"gradio\"])\n",
        "  import gradio as gr\n",
        "\n",
        "from os.path import exists as path_exists\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
        "\n",
        "class RepoHandler:\n",
        "    def __init__(self, repo_url: str) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the RepoHandler class with the provided repository URL and requirements file.\n",
        "        Args:\n",
        "        - repo_url: URL of the git repository to clone.\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        self.is_google_colab = False\n",
        "        self.fix_for_gr, self.fix_for_st = None, None\n",
        "        self.app_file = \"app.py\"\n",
        "        if 'google.colab' in str(get_ipython()):\n",
        "            print(f'{bold}Running on: \"Google Colab\"{unbold}')\n",
        "            self.is_google_colab = True\n",
        "        else:\n",
        "            print(f'{bold}Running on: Local or \"SM Studio Lab\"{unbold}')\n",
        "        self.repo_url = repo_url\n",
        "        self.repo_name = self.repo_url.split('/')[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        if os.path.exists(self.repo_name):\n",
        "            return self.retrieve_readme(f'{self.repo_name}/README.md')\n",
        "        else:\n",
        "            print(f\"{bold}The repo '{self.repo_name}' has not been cloned yet.{unbold}\")\n",
        "            return None\n",
        "            \n",
        "    def retrieve_readme(self, filename) -> Dict:\n",
        "        readme = {}\n",
        "        if path_exists(filename):\n",
        "            with open(filename) as f:\n",
        "                for line in f:\n",
        "                    if not line.find(':') > 0 or 'Check' in line: continue\n",
        "                    (k,v) = line.split(':')\n",
        "                    readme[(k)] = v.strip().replace('\\n','')\n",
        "        else:\n",
        "            print(f\"{bold}No 'readme.md' file{unbold}\")\n",
        "            \n",
        "        return readme\n",
        "        \n",
        "    def clone_repo(self, overwrite=False) -> None:\n",
        "        \"\"\"\n",
        "        Clone the git repository specified in the repo_url attribute.\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        # Check if repository has already been cloned locally\n",
        "        if overwrite and os.path.exists(self.repo_name): \n",
        "            try:\n",
        "                shutil.rmtree(self.repo_name)\n",
        "            except OSError as e:\n",
        "                print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
        "        if os.path.exists(self.repo_name):\n",
        "            print(f\"{bold}Repository '{self.repo_name}' has already been cloned.{unbold}\")\n",
        "        else:\n",
        "            print(f\"{bold}Cloning repo... may take a few minutes... remember to set your Space to 'public'...{unbold}\")\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"install\", \"git-lfs\"])\n",
        "            subprocess.run([\"git\", \"lfs\", \"install\", \"--system\", \"--skip-repo\"])\n",
        "            subprocess.run([\"git\", \"clone\", self.repo_url])\n",
        "\n",
        "    def install_requirements(self, requirements_file: str = None, install_xformers: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Install the requirements specified in the requirements_file attribute.\n",
        "        \n",
        "        Args:\n",
        "        - requirements_file: Name of the file containing the requirements to install. This file must be \n",
        "        located in the root directory of the repository. Defaults to \"requirements.txt\".\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        if not requirements_file: requirements_file = f\"{self.repo_name}/requirements.txt\"\n",
        "        \n",
        "        # install requirements\n",
        "        print(f\"{bold}Installing requirements... may take a few minutes...{unbold}\")\n",
        "        subprocess.run([\"pip\", \"install\", \"-r\", requirements_file])\n",
        "        if install_xformers: self.install_xformers()\n",
        "\n",
        "    def run_web_demo(self, aws_domain=None, aws_region=None) -> None:\n",
        "        \"\"\"\n",
        "        Launch the Gradio or Streamlit web demo for the cloned repository.\n",
        "        Works with Google Colab or SageMaker Studio Lab.\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        import torch\n",
        "        if torch.cuda.is_available(): print(f\"{bold}Using: {unbold}{self.get_gpu_memory_map()}\")\n",
        "        else: print(f\"{bold}Not using the GPU{unbold}\")\n",
        "        \n",
        "        readme = self.__str__()\n",
        "        self.app_file = readme[\"app_file\"]\n",
        "        print(f\"{bold}Demo: `{readme['title']}`{newline}{unbold}\")\n",
        "        print(f\"{bold}Downloading models... might take up to 10 minutes to finish...{unbold}\")\n",
        "        print(f\"{bold}Once finished, click the link below to open your application (in SM Studio Lab):{newline}{unbold}\")\n",
        "        if all([aws_domain, aws_region]):\n",
        "              print(f'{bold}https://{aws_domain}.studio.{aws_region}.sagemaker.aws/studiolab/default/jupyter/proxy/6006/{unbold}')\n",
        "        \n",
        "        if readme[\"sdk\"] == 'gradio':\n",
        "            gr.close_all()\n",
        "            if not self.is_google_colab:\n",
        "                !export GRADIO_SERVER_PORT=6006 && cd $self.repo_name && python $self.app_file\n",
        "                # os.system(f'export GRADIO_SERVER_PORT=6006 && cd {self.repo_name} && python {readme[\"app_file\"]}')\n",
        "            else:\n",
        "                new_filename = self.replace_gradio_launcher(f'{self.repo_name}/{readme[\"app_file\"]}')\n",
        "                !cd $self.repo_name && python $new_filename\n",
        "                # os.system(f'cd {self.repo_name} && python {readme[\"app_file\"]}')\n",
        "        elif readme[\"title\"] == 'streamlit':\n",
        "            if not self.is_google_colab:\n",
        "                !cd $self.repo_name && streamlit run $self.app_file --server.port 6006\n",
        "                # os.system(f'cd {self.repo_name} && streamlit run {readme[\"app_file\"]} --server.port 6006')\n",
        "            else:\n",
        "                !cd $self.repo_name && streamlit run $self.app_file\n",
        "                # os.system(f'cd {self.repo_name} && streamlit run {readme[\"app_file\"]}')\n",
        "        else:\n",
        "            print('This notebook will not work with static apps hosted on \"Spaces\"')\n",
        "\n",
        "    def get_gpu_memory_map(self) -> Dict[str, int]:\n",
        "        \"\"\"Get the current gpu usage.\n",
        "        Return:\n",
        "            A dictionary in which the keys are device ids as integers and\n",
        "            values are memory usage as integers in MB.\n",
        "        \"\"\"\n",
        "        result = subprocess.run(\n",
        "            [\"nvidia-smi\", \"--query-gpu=name,memory.total,memory.free\", \"--format=csv,noheader\",],\n",
        "            encoding=\"utf-8\",\n",
        "            # capture_output=True,          # valid for python version >=3.7\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,  # for backward compatibility with python version 3.6\n",
        "            check=True,\n",
        "        )\n",
        "        # Convert lines into a dictionary, return f\"{}\"\n",
        "        gpu_memory = [x for x in result.stdout.strip().split(os.linesep)]\n",
        "        gpu_memory_map = {f\"gpu_{index}\": memory for index, memory in enumerate(gpu_memory)}\n",
        "        \n",
        "        return gpu_memory_map\n",
        "\n",
        "    def replace_gradio_launcher(self, old_filename) -> str:\n",
        "        # Read the contents of the file\n",
        "        with open(old_filename, \"r\") as f:\n",
        "            contents = f.read()\n",
        "        # Define the regular expression pattern\n",
        "        pattern = r\"\\.launch\\((.*?)\\)\"\n",
        "        # Use the sub method to replace the text\n",
        "        contents = re.sub(pattern, \".launch(share=True)\", contents)\n",
        "        # Write the modified contents back to the file\n",
        "        new_filename = Path(old_filename).parent / f\"{Path(old_filename).stem}_modified.py\"\n",
        "        with open(new_filename, \"w\") as f:\n",
        "            f.write(contents)\n",
        "\n",
        "        return new_filename.name\n",
        "\n",
        "    def install_xformers(self) -> None:\n",
        "        from subprocess import getoutput\n",
        "        from IPython.display import HTML\n",
        "        from IPython.display import clear_output\n",
        "        import time\n",
        "\n",
        "        subprocess.run([\"pip\", \"install\", \"-U\", \"--pre\", \"triton\"])\n",
        "\n",
        "        s = getoutput('nvidia-smi')\n",
        "        if 'T4' in s: gpu = 'T4'\n",
        "        elif 'P100' in s: gpu = 'P100'\n",
        "        elif 'V100' in s: gpu = 'V100'\n",
        "        elif 'A100' in s: gpu = 'A100'\n",
        "\n",
        "        while True:\n",
        "            try: \n",
        "                gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'\n",
        "                break\n",
        "            except:\n",
        "                pass\n",
        "            print(f'{bold} Seems that your GPU is not supported at the moment.{unbold}')\n",
        "            time.sleep(5)\n",
        "\n",
        "        if (gpu=='T4'): \n",
        "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
        "        elif (gpu=='P100'): \n",
        "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
        "        elif (gpu=='V100'): \n",
        "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
        "        elif (gpu=='A100'): \n",
        "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
        "\n",
        "        subprocess.run([\"pip\", \"install\", \"-q\", precompiled_wheels])"
      ],
      "metadata": {
        "id": "rmBjvPVHO1fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ya tenemos todo listo, empezar el proceso! ðŸš€"
      ],
      "metadata": {
        "id": "ioW5IDk4PXg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = RepoHandler(\n",
        "    repo_url='https://huggingface.co/spaces/Fantasy-Studio/Paint-by-Example'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvACWIthPn4P",
        "outputId": "9ce736c1-5ef8-4737-e48b-8b27fa0c6797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mRunning on: \"Google Colab\"\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8OckZ3IQdola"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app.clone_repo(overwrite=True)\n",
        "app.install_requirements()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfE0Zx-2PvUd",
        "outputId": "073e29e0-40ba-4d47-d816-d0dceb98b6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mCloning repo... may take a few minutes... remember to set your Space to 'public'...\u001b[0m\n",
            "\u001b[1mInstalling requirements... may take a few minutes...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app.run_web_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tkTJI0SPvv5",
        "outputId": "e69a7612-5556-40c9-ada8-3bbac606f8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mUsing: \u001b[0m{'gpu_0': 'Tesla T4, 15109 MiB, 15106 MiB'}\n",
            "\u001b[1mDemo: `Paint by example`\n",
            "\u001b[0m\n",
            "\u001b[1mDownloading models... might take up to 10 minutes to finish...\u001b[0m\n",
            "\u001b[1mOnce finished, click the link below to open your application (in SM Studio Lab):\n",
            "\u001b[0m\n",
            "Fetching 9 files: 100% 9/9 [00:00<00:00, 14287.94it/s]\n",
            "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
            "```\n",
            "pip install accelerate\n",
            "```\n",
            ".\n",
            "You are using a model of type clip_vision_model to instantiate a model of type clip. This is not supported for all configurations of models and can yield errors.\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://1dec275ab6ee02d2.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n",
            "100% 50/50 [00:09<00:00,  5.45it/s]\n",
            "100% 50/50 [00:21<00:00,  2.35it/s]\n",
            "100% 50/50 [00:21<00:00,  2.28it/s]\n",
            "100% 50/50 [00:12<00:00,  4.05it/s]\n",
            "100% 50/50 [00:14<00:00,  3.48it/s]\n",
            "100% 50/50 [00:14<00:00,  3.41it/s]\n",
            "100% 50/50 [00:09<00:00,  5.31it/s]\n",
            "100% 50/50 [00:21<00:00,  2.32it/s]\n",
            "100% 50/50 [00:22<00:00,  2.27it/s]\n",
            "100% 50/50 [00:22<00:00,  2.24it/s]\n",
            "100% 50/50 [00:21<00:00,  2.33it/s]\n",
            "100% 50/50 [00:17<00:00,  2.91it/s]\n",
            "100% 50/50 [00:17<00:00,  2.86it/s]\n",
            "100% 50/50 [00:17<00:00,  2.88it/s]\n",
            "100% 50/50 [00:09<00:00,  5.25it/s]\n",
            "100% 50/50 [00:09<00:00,  5.29it/s]\n",
            "100% 50/50 [00:16<00:00,  2.95it/s]\n",
            "100% 50/50 [00:17<00:00,  2.92it/s]\n",
            "100% 50/50 [00:09<00:00,  5.26it/s]\n",
            "100% 50/50 [00:09<00:00,  5.29it/s]\n",
            "100% 50/50 [00:14<00:00,  3.47it/s]\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1582, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"app_modified.py\", line 157, in <module>\n",
            "    image_blocks.launch(share=True)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1504, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1584, in block_thread\n",
            "    print(\"Keyboard interruption in main thread... closing server.\")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}